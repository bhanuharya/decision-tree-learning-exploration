{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a80ca871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5ae487c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fd408d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_breast_cancer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\LilyRaid\\Documents\\ML\\decision-tree-learning-exploration\\.ipynb_checkpoints\\model-checkpoint.ipynb Cell 3'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LilyRaid/Documents/ML/decision-tree-learning-exploration/.ipynb_checkpoints/model-checkpoint.ipynb#ch0000002?line=0'>1</a>\u001b[0m \u001b[39m#Preparing data\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/LilyRaid/Documents/ML/decision-tree-learning-exploration/.ipynb_checkpoints/model-checkpoint.ipynb#ch0000002?line=1'>2</a>\u001b[0m breast_cancer \u001b[39m=\u001b[39m load_breast_cancer()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LilyRaid/Documents/ML/decision-tree-learning-exploration/.ipynb_checkpoints/model-checkpoint.ipynb#ch0000002?line=2'>3</a>\u001b[0m data_play_tennis  \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mplay_tennis.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LilyRaid/Documents/ML/decision-tree-learning-exploration/.ipynb_checkpoints/model-checkpoint.ipynb#ch0000002?line=4'>5</a>\u001b[0m \u001b[39m#convert breast cancer data to pandas dataframe\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_breast_cancer' is not defined"
     ]
    }
   ],
   "source": [
    "#Preparing data\n",
    "breast_cancer = load_breast_cancer()\n",
    "data_play_tennis  = pd.read_csv('play_tennis.csv')\n",
    "\n",
    "#convert breast cancer data to pandas dataframe\n",
    "data_breast_cancer = pd.DataFrame(breast_cancer['data'], columns=breast_cancer['feature_names'])\n",
    "data_breast_cancer['target'] = breast_cancer['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f557017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Breast Cancer Data Head\n",
    "data_breast_cancer.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64096366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play Tennis Data Head\n",
    "data_play_tennis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fef9a4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split play_tennis to training and testing data\n",
    "y = data_play_tennis.play\n",
    "x = data_play_tennis.drop('play', axis = 1)\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)\n",
    "\n",
    "print(\"shape of original dataset :\", data_play_tennis.shape)\n",
    "print(\"shape of input - training set\", x_train.shape)\n",
    "print(\"shape of output - training set\", y_train.shape)\n",
    "print(\"shape of input - testing set\", x_test.shape)\n",
    "print(\"shape of output - testing set\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df54f48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM algorithm\n",
    "plt.scatter()\n",
    "plt.show()\n",
    "\n",
    "clf = svm.SVC(kernel='linear',C= 1.0)\n",
    "clf.fit(a,b)\n",
    "\n",
    "w = clf.coef_[0]\n",
    "print(w)\n",
    "\n",
    "f = -w[0] / w[1]\n",
    "\n",
    "xx = np.linspace(0,12)\n",
    "yy = f * xx - clf.intercept_[0] / w[1]\n",
    "\n",
    "h0 = plt.plot(xx, yy, 'k-', label=\"non weighted div\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e14fa1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of original dataset : (14, 6)\n",
      "shape of input - training set (11, 5)\n",
      "shape of output - training set (11,)\n",
      "shape of input - testing set (3, 5)\n",
      "shape of output - testing set (3,)\n"
     ]
    }
   ],
   "source": [
    "#K means Clustering\n",
    "dataset = pd.read_csv('play_tennis.csv')\n",
    "X = dataset.iloc[:, [3, 4]].values\n",
    "# y = dataset.iloc[:, 3].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "sc_y = StandardScaler()\n",
    "y_train = sc_y.fit_transform(y_train)\n",
    "\n",
    "# Using the elbow method to find the optimal number of clusters\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Fitting K-Means to the dataset\n",
    "kmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 42)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# Visualising the clusters\n",
    "plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\n",
    "plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\n",
    "plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\n",
    "plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\n",
    "plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')\n",
    "plt.title('Clusters of customers')\n",
    "plt.xlabel('Annual Income (k$)')\n",
    "plt.ylabel('Spending Score (1-100)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d2a30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network Algorithm\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, learning_rate):\n",
    "        self.weights = np.array([np.random.randn(), np.random.randn()])\n",
    "        self.bias = np.random.randn()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def _sigmoid_deriv(self, x):\n",
    "        return self._sigmoid(x) * (1 - self._sigmoid(x))\n",
    "\n",
    "    def predict(self, input_vector):\n",
    "        layer_1 = np.dot(input_vector, self.weights) + self.bias\n",
    "        layer_2 = self._sigmoid(layer_1)\n",
    "        prediction = layer_2\n",
    "        return prediction\n",
    "\n",
    "    def _compute_gradients(self, input_vector, target):\n",
    "        layer_1 = np.dot(input_vector, self.weights) + self.bias\n",
    "        layer_2 = self._sigmoid(layer_1)\n",
    "        prediction = layer_2\n",
    "\n",
    "        derror_dprediction = 2 * (prediction - target)\n",
    "        dprediction_dlayer1 = self._sigmoid_deriv(layer_1)\n",
    "        dlayer1_dbias = 1\n",
    "        dlayer1_dweights = (0 * self.weights) + (1 * input_vector)\n",
    "\n",
    "        derror_dbias = (\n",
    "            derror_dprediction * dprediction_dlayer1 * dlayer1_dbias\n",
    "        )\n",
    "        derror_dweights = (\n",
    "            derror_dprediction * dprediction_dlayer1 * dlayer1_dweights\n",
    "        )\n",
    "\n",
    "        return derror_dbias, derror_dweights\n",
    "\n",
    "    def _update_parameters(self, derror_dbias, derror_dweights):\n",
    "        self.bias = self.bias - (derror_dbias * self.learning_rate)\n",
    "        self.weights = self.weights - (\n",
    "            derror_dweights * self.learning_rate\n",
    "        )\n",
    "\n",
    "    def train(self, input_vectors, targets, iterations):\n",
    "        cumulative_errors = []\n",
    "        for current_iteration in range(iterations):\n",
    "            # Pick a data instance at random\n",
    "            random_data_index = np.random.randint(len(input_vectors))\n",
    "\n",
    "            input_vector = input_vectors[random_data_index]\n",
    "            target = targets[random_data_index]\n",
    "\n",
    "            # Compute the gradients and update the weights\n",
    "            derror_dbias, derror_dweights = self._compute_gradients(\n",
    "                input_vector, target\n",
    "            )\n",
    "\n",
    "            self._update_parameters(derror_dbias, derror_dweights)\n",
    "\n",
    "            # Measure the cumulative error for all the instances\n",
    "            if current_iteration % 100 == 0:\n",
    "                cumulative_error = 0\n",
    "                # Loop through all the instances to measure the error\n",
    "                for data_instance_index in range(len(input_vectors)):\n",
    "                    data_point = input_vectors[data_instance_index]\n",
    "                    target = targets[data_instance_index]\n",
    "\n",
    "                    prediction = self.predict(data_point)\n",
    "                    error = np.square(prediction - target)\n",
    "\n",
    "                    cumulative_error = cumulative_error + error\n",
    "                cumulative_errors.append(cumulative_error)\n",
    "\n",
    "        return cumulative_errors\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
